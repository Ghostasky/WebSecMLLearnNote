[toc]

# 第5章 K近邻算法

可以理解距离接近的事物具有相同属性的可能性 要大于距离相对较远的，这个也是K近邻算法的核心思想。

K近邻（K-Nearest Neighbor，KNN）

所谓KNN，就是K个最近的邻居的意思，说的是每个样 本都可以用它最接近的K个邻居来代表。KNN算法的核心思想是：如果 一个样本在特征空间中的K个最相邻的样本中的大多数属于某一个类 别，则该样本也属于这个类别，并具有这个类别上样本的特性，

KNN常用的算法为： 

-   Brute Force； 
-   K-D Tree； 
-   Ball Tree。

## 5.2 hello world！K近邻



```py
import numpy as np
from sklearn.neighbors import NearestNeighbors
print(__doc__)


X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
distances, indices = nbrs.kneighbors(X)
# 返回值indices：第0列元素为参考点的索引，后面是(n_neighbors - 1)个与之最近的点的索引
# 返回值distances：第0列元素为与自身的距离(为0)，后面是(n_neighbors - 1)个与之最近的点与参考点的距离
print(distances)
print(indices)
print(nbrs.kneighbors_graph(X).toarray())

```

监督学习

```py
from sklearn.neighbors import KNeighborsClassifier
X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
neigh = KNeighborsClassifier(n_neighbors=2)
neigh.fit(X, y)
print(neigh.predict([[2.1]]))
print(neigh.predict_proba([[0.1]]))
```

## 5.3 示例：使用K近邻算法检测异常操作



5.3示例中只比较了最频繁和最不频繁的操作命令，我们这次尝试 进行全量比较



## 5.5 示例：使用K近邻算法检测Rootkit

```py
v=load_kdd99("../../Data/kddcup99/corrected")
print(type(v))
x,y=get_rootkit2andNormal(v)
print(len(x),len(y))
clf = KNeighborsClassifier(n_neighbors=3)
a = cross_val_score(clf, x, y, n_jobs=-1, cv=10)
print(a)
print(np.mean(a))
```

## 5.6 示例：使用K近邻算法检测WebShell





